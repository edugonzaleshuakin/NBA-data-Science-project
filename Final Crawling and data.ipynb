{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaA4T69DeK9l"
   },
   "source": [
    "<h1><b>Crawling & Scrawling Data</h1></b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1643194099079,
     "user": {
      "displayName": "גיא דוד",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06047182624448228798"
     },
     "user_tz": -120
    },
    "id": "HayELJ39m7C7"
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import math\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1643194113600,
     "user": {
      "displayName": "גיא דוד",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06047182624448228798"
     },
     "user_tz": -120
    },
    "id": "TbccXmMZm7DA"
   },
   "outputs": [],
   "source": [
    "def generate_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")    \n",
    "    #Crawl-delay: 3\n",
    "    time.sleep(3)\n",
    "    links=soup.find_all(\"a\")\n",
    "    list_of_links=[]\n",
    "    for i,a in enumerate(links):\n",
    "        string=a.get('href')\n",
    "        time.sleep(3)\n",
    "        if (\"/draft/NBA_\" in string)  :\n",
    "            suffix=string.replace(\"/draft\",\"\")\n",
    "            list_of_links.append(url+suffix)  \n",
    "            if \"1970\" in string:\n",
    "                break                  \n",
    "    return list_of_links        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1643194117129,
     "user": {
      "displayName": "גיא דוד",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06047182624448228798"
     },
     "user_tz": -120
    },
    "id": "69F5ozRHm7DB"
   },
   "outputs": [],
   "source": [
    "def get_properties(url):\n",
    "    pk=[]\n",
    "    team=[]\n",
    "    names=[]\n",
    "    college=[]\n",
    "    seasons=[]\n",
    "    games=[]\n",
    "    minutes=[]\n",
    "    points=[]\n",
    "    ttlribounds=[]\n",
    "    asists=[]\n",
    "    FGper=[]\n",
    "    threePper=[]\n",
    "    FTper=[]\n",
    "    MP=[]\n",
    "    PntperGame=[]\n",
    "    ttlriboundsperGame=[]\n",
    "    asistsperGame=[]\n",
    "    WS=[]\n",
    "    WSminutes=[]\n",
    "    BPM=[]\n",
    "    vorp=[]\n",
    "    player_links=[]\n",
    "    years=[]\n",
    "    response = requests.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    #the number of <tr> within <table> \n",
    "    rows=soup.table.tbody.find_all(\"tr\")\n",
    "    #it takes 2 rounds inorder to compleate the draft \n",
    "    #each year has diffarent number of players to pick from --> difarent number of <tr> each <table>\n",
    "    rounds=math.floor((len(rows)/2)-1)     \n",
    "    for i,r in enumerate(rows):\n",
    "                years.append(int(url[47:51]))\n",
    "                #at some years like 2001 there are missing rows, \"Minnesota forfeited the 29th overall pick.\"\n",
    "                if r.find(\"td\",attrs={\"class\":\"center\"}):\n",
    "                    continue\n",
    "                if (i < rounds or i>rounds+1) and (r.find_all(\"a\",attrs={\"href\":re.compile(\"player\")})) :\n",
    "                    player_links.append(url[:36]+r.find_all(\"a\",attrs={\"href\":re.compile(\"player\")})[0].get(\"href\")) #players_links \n",
    "                    l = r.find_all(\"td\")\n",
    "                    pk.append(l[0].string)\n",
    "                    team.append(l[1].string)\n",
    "                    names.append(l[2].string)\n",
    "                    college.append(l[3].string)\n",
    "                    seasons.append(l[4].string)\n",
    "                    games.append(l[5].string)\n",
    "                    minutes.append(l[6].string)\n",
    "                    points.append(l[7].string)\n",
    "                    ttlribounds.append(l[8].string)\n",
    "                    asists.append(l[9].string)\n",
    "                    FGper.append(l[10].string)\n",
    "                    threePper.append(l[11].string)\n",
    "                    FTper.append(l[12].string)\n",
    "                    MP.append(l[13].string)\n",
    "                    PntperGame.append(l[14].string)\n",
    "                    ttlriboundsperGame.append(l[15].string)\n",
    "                    asistsperGame.append(l[16].string)\n",
    "                    WS.append(l[17].string)\n",
    "                    WSminutes.append(l[18].string)\n",
    "                    BPM.append(l[19].string)\n",
    "                    vorp.append(l[20].string)\n",
    "                        \n",
    "                                                 \n",
    "    return  pk,team, names, college ,seasons ,games ,minutes ,points, ttlribounds, asists, FGper, threePper, FTper, MP, PntperGame, ttlriboundsperGame, asistsperGame, WS, WSminutes, BPM ,vorp,player_links,years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1643194124050,
     "user": {
      "displayName": "גיא דוד",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06047182624448228798"
     },
     "user_tz": -120
    },
    "id": "MBmwZz6WeK9-"
   },
   "outputs": [],
   "source": [
    "def generate_player_data(df):\n",
    "    \n",
    "    players_links=[]\n",
    "    players_names=[]\n",
    "    players_height=[]\n",
    "    players_weight=[]\n",
    "    players_position=[]\n",
    "    \n",
    "    \n",
    "    players_names.extend(df['Names'].tolist())\n",
    "    players_links.extend(df['player page'].tolist())\n",
    "    \n",
    "        \n",
    "    #all players positions\n",
    "    for link in players_links:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        try1 = soup.find(\"strong\",string=re.compile(\"Position:\")).next.next.split(\"  \")[1].rstrip()\n",
    "        players_position.append(try1)\n",
    "        \n",
    "        #Gets Height\n",
    "        res1=soup.body.find_all(\"span\")\n",
    "        \n",
    "        \n",
    "        if ')' in res1[9].text:  #Some players don't have full data\n",
    "            players_height.append(\"0\")\n",
    "            players_weight.append(\"0\")\n",
    "            continue\n",
    "        \n",
    "        players_height.append(res1[9].text)\n",
    "        \n",
    "        #Gets Weight\n",
    "        res2=soup.body.find_all(\"span\")\n",
    "        players_weight.append(res2[10].text)\n",
    "        \n",
    "        \n",
    "    #turns Height 6-5 to 6.5\n",
    "    for i in range(len(players_height)):\n",
    "        players_height[i] = players_height[i].replace(\"-\", \".\")\n",
    "\n",
    "    #Creat new dataframe with data\n",
    "    data=pd.DataFrame(list(zip(players_names, players_height, players_weight,players_position)), columns=[\"Names\", \"Height\", \"Weight\",\"Position\"])\n",
    "    \n",
    "    #clean Height & Weight\n",
    "    data['Height'] = data['Height'].astype(float)\n",
    "    data['Height'] = 30.01818181818182 * data['Height']\n",
    "    data['Weight'] = data['Weight'].str.replace('lb', '')\n",
    "    data['Weight'] = data['Weight'].astype(float)\n",
    "    data['Weight'] = 0.453592 * data['Weight']\n",
    "    \n",
    "    \n",
    "    #split positions\n",
    "    new = data[\"Position\"].str.split(\"and\", n = 1, expand = True)\n",
    "    data[\"First Position\"]= new[0]\n",
    "    #data[\"Second Position\"]= new[1]\n",
    "\n",
    "\n",
    "    #Cleans first position from ',' or 'and'\n",
    "    new2 = data[\"First Position\"].str.split(\",\", n = 1, expand = True)\n",
    "    data[\"First Position\"]= new2[0]\n",
    "    \n",
    "    #Cleans first position from ' '\n",
    "    data[\"First Position\"] = data[\"First Position\"].str.strip()\n",
    "    \n",
    "    #Merge with original data\n",
    "    df_final = pd.merge(df, data, how='inner', on = 'Names')\n",
    "    \n",
    "    \n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_zero_pages_names(df):\n",
    "    df = df[df['Height'] != 0]\n",
    "    df = df.drop(['player page','Names','Position'], axis=1)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPz0wzJweK-C"
   },
   "source": [
    "<h2><b> crawling links </b></h2>\n",
    "\n",
    "Robots.txt request a 3 sec crawl-delay and the mount of Data thet we needed for our project therefore the Data collection process can take nearly an hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10373,
     "status": "ok",
     "timestamp": 1643194987227,
     "user": {
      "displayName": "גיא דוד",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06047182624448228798"
     },
     "user_tz": -120
    },
    "id": "8hMiPjkXm7DE"
   },
   "outputs": [],
   "source": [
    "links=[]\n",
    "links.extend(generate_links(\"https://www.basketball-reference.com/draft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMF7XqMTeK-G"
   },
   "source": [
    "<b> collecting properties and arrange in DataFrame </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 6558966,
     "status": "error",
     "timestamp": 1643202290973,
     "user": {
      "displayName": "גיא דוד",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06047182624448228798"
     },
     "user_tz": -120
    },
    "id": "bN1ym2Q8eK-I",
    "outputId": "c010caee-3bfa-430c-b45b-5be84643fa4e"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "for i,link in enumerate(links) :\n",
    "    pk,team, names, college ,seasons ,games ,minutes ,points, ttlribounds, asists, FGper, threePper, FTper, MP, PntperGame, ttlriboundsperGame, asistsperGame, WS, WSminutes, BPM ,vorp,player_links,years = get_properties(link)\n",
    "    df=pd.DataFrame(list(zip(pk,team,player_links,years,names, college,seasons ,games ,minutes ,points, ttlribounds, asists, FGper, threePper, FTper, MP, PntperGame, ttlriboundsperGame, asistsperGame, WS, WSminutes, BPM ,vorp)), columns=['Overall Pick','Teams','player page','year', 'Names','College','Seasons in NBA', '# Games','Minutes Played','Points','Total Rebounds','Assists','Field Goal %','3 Point Field Goal %', 'Free Throw %', 'Minutes Played per Game','Points per Game', 'Total Rebounds per Game', 'Assists per Game','Win Shares','Win Shares per 48 Minutes','Box +-','VORP'])\n",
    "    df2=pd.concat([df2,df],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0CCqEZ_eK-N"
   },
   "source": [
    "<b>the pre-processed DataFrame</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0qGHncGeK-Q"
   },
   "source": [
    "<b>cleanning and processing Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1643195727295,
     "user": {
      "displayName": "גיא דוד",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06047182624448228798"
     },
     "user_tz": -120
    },
    "id": "pnUUfFOBeK-R"
   },
   "outputs": [],
   "source": [
    "df2.dropna(axis=0,inplace=True)\n",
    "df3=generate_player_data(df2.copy())\n",
    "df4 = clean_zero_pages_names(df3.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"machine_learning_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jfSa1yJeK-V"
   },
   "source": [
    "<h3><b>collecting salaries</b></h3>\n",
    "\n",
    "<p>inorder to collect salaries from the datasource wi nedd to crawl each player_page , searching for the first year at the salaries table in it. we vlidate that the salary is ctually the first one by matching the draft year from dataframe and the salary year from player_page </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_salary(df):\n",
    "    salaries=[]\n",
    "    for i,row in df.iterrows():\n",
    "        response = requests.get(row[2])\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")      \n",
    "        l=soup.find_all(\"div\")\n",
    "        time.sleep(3)\n",
    "        li=str(l[0]).split(\"<tr >\")\n",
    "        for k in li:\n",
    "                flag=0\n",
    "                if (str(row[3]) in k) and (\"csk=\" in k):\n",
    "                    for i in k.split(\"csk=\",2)[len(k.split(\"csk=\",2))-1][:-20].split():\n",
    "                        if i.strip('\"\"').isdigit() :\n",
    "                            num=int(i.strip('\"\"'))\n",
    "                            if num>100000:\n",
    "                                salaries.append(num)\n",
    "                                flag=1\n",
    "                            break\n",
    "                    break\n",
    "        if flag==0:\n",
    "            salaries.append(float(\"NaN\"))       \n",
    "    return salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhe72SK6m7DF"
   },
   "outputs": [],
   "source": [
    "li=[]\n",
    "li.extend(generate_first_salary(df3.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFvblH8ceK-Y"
   },
   "outputs": [],
   "source": [
    "df5[\"salary\"]=li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hGk3vi-eK-Z"
   },
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xS2GLDYaeK-a"
   },
   "outputs": [],
   "source": [
    "df5.dropna(axis=0).to_csv(\"Full_Data_with_salaries.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Crawling and data(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
